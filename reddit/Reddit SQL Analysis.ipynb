{"cells":[{"cell_type":"markdown","source":["## Analysis of Reddit Comments\nI pulled the Dataset from [Reddit's Archive Site](https://archive.org/details/2015_reddit_comments_corpus), which contains a \"_Complete Public Reddit Comments Corpus_\".  \nI'll attempt to bring the dataset into my environment, perform an ETL on the dataset, and run LDA on it to determine the topics of them."],"metadata":{}},{"cell_type":"markdown","source":["## Read Reddit Parquet Data For Analysis\nWe've pulled the dataset into an S3 bucket to allow Spark to process it further.  \n* **Input**: json + bzip2 compression (50,687,364,160 = 50.5GB)\n* **Output**: Parquet + gzip (52,395,455,189 = 52.3GB)"],"metadata":{}},{"cell_type":"code","source":["spark.read.parquet(\"/mnt/mwc/reddit_year_p/year=2012/\").count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# List partitions and register as temp tables\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n# To create the complete dataset, let's create temporary tables per year then find create a master union table\ndf = sc.parallelize(dbutils.fs.ls(\"/mnt/mwc/reddit_year_p\")).toDF()\n\n# Parse the year partition to get an array of years to register the tables by\nyears = df.select(F.regexp_extract('name', '(\\d+)', 1).alias('year')).collect()\nyear_partitions = [x.asDict().values()[0] for x in years if x.asDict().values()[0]]\nyear_partitions\n\n# Loop over and register a table per year \nfor y in year_partitions:\n  df = sqlContext.read.parquet(\"/mnt/mwc/reddit_year_p/year=%s\" % y)\n  df.createOrReplaceTempView(\"reddit_%s\" % y)\n\n# Register the root directory for the complete dataset \ndf_complete = spark.read.parquet(\"/mnt/mwc/reddit_year_p/\")\n# df_complete.createGlobalTempView(\"reddit_all\")\ndf_complete.createOrReplaceTempView(\"reddit_all\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df_complete.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark.sql(\"select * from reddit_all where year = 2014\").explain(True)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql\nselect * from reddit_all where year = $arg"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df_complete.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(df_complete)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["A few ideas of what can be accomplished with this dataset \n* Identify and track topics associated with every subreddit and username\n* Model flow of conversations (e.g. rate of replies compared to controversiality of comment/post)\n* Predict posts/subreddits a user will next engage with (i.e. recommender systems)"],"metadata":{}},{"cell_type":"code","source":["%sh\nls -lh /dbfs/mnt/mwc/reddit_year_p/year=2014/"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\n-- run 1\nselect count(1) as count, year from reddit_all group by year order by year asc ; "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql\n-- run 2 \nselect count(1) as count, year from reddit_all group by year order by year asc ; "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Total Number of Comments posted per day of week in 2012"],"metadata":{}},{"cell_type":"code","source":["%sql\n--- Find the number of comments per day of week for 2012\nSELECT day, sum(comments) as counts from (\n  SELECT date_format(from_unixtime(created_utc), 'EEEE') day, COUNT(*) comments\n  FROM reddit_2014\n  GROUP BY created_utc\n  ORDER BY created_utc\n) q2\nGROUP BY day \nORDER BY counts; "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\n--- Find the number of comments per day of week for 2012\nSELECT day, sum(comments) as counts from (\n  SELECT date_format(from_unixtime(created_utc), 'EEEE') day, COUNT(*) comments\n  FROM reddit_2014\n  GROUP BY created_utc\n  ORDER BY created_utc\n) q2\nGROUP BY day \nORDER BY counts; "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Best Time to Comment on Posts in 2014\nTo view the SQL language manaual in [Databricks Docs](https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html)"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Select best time to comment on posts \nCREATE TABLE IF NOT EXISTS popular_posts_2014\n  USING parquet \n  OPTIONS (\n    path \"/mnt/mwc/popular_posts_2014\"\n  ) \nAS SELECT \n  day,\n  hour,\n  SUM(IF(score >= 1000, 1, 0)) as score_gt_1k\nFROM\n  (SELECT \n    date_format(from_utc_timestamp(from_unixtime(created_utc), \"PST\"), 'EEEE') as day, \n    date_format(from_utc_timestamp(from_unixtime(created_utc), \"PST\"), 'h a') as hour,\n    score\n  FROM reddit_2014) q1\nGROUP BY day, hour\nORDER BY day, hour"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["current_table = 'popular_posts_2014'\ndf = spark.read.parquet(\"/mnt/mwc/popular_posts_2014\")\ndf.createOrReplaceTempView('popular_posts_2014')\ndisplay(table(current_table))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Matplotlib Visualization"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Define the labels sorted in my predefined order\ncolumn_labels = [\"12 AM\", \"1 AM\", \"2 AM\", \"3 AM\", \"4 AM\", \"5 AM\", \"6 AM\", \"7 AM\", \"8 AM\", \"9 AM\", \"10 AM\", \"11 AM\", \"12 PM\", \"1 PM\", \"2 PM\", \"3 PM\", \"4 PM\", \"5 PM\", \"6 PM\", \"7 PM\", \"8 PM\", \"9 PM\", \"10 PM\", \"11 PM\"]\n\n# Zip up the 2 column names by predefined order\ncolumn2_name = ['Count of Comments > 1K Votes']*len(column_labels)\ncolumn_label_sorted = zip(column2_name, column_labels)\n\n# Define the row labels to map the calendar week\nrow_labels = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n\ndata = [[x.day, x.hour, x.score_gt_1k] for x in table(current_table).collect()]"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Create the Pivot Table\ncolNames = ['Day of Week', 'Hour', 'Count of Comments > 1K Votes']\ndata_m = pd.DataFrame(data,columns = colNames)\npvt = pd.pivot_table(data_m, index=['Day of Week'], columns=['Hour'])"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Call reindex_axis to sort the row axis by my order array\n# Call reindex_axis on axis=1 (columns) to sort columns by my ordered zipped array\npvt_sorted = pvt.reindex_axis(row_labels, axis=0).reindex_axis(column_label_sorted, axis=1)\npvt_sorted "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["data_p = pvt_sorted.as_matrix().transpose()\nfig, ax = plt.subplots()\nheatmap = ax.pcolor(data_p, cmap=plt.cm.Blues)\n\n# put the major ticks at the middle of each cell\nax.set_xticks(np.arange(data_p.shape[1])+0.5, minor=False)\nax.set_yticks(np.arange(data_p.shape[0])+0.5, minor=False)\n\n# want a more natural, table-like display\nax.invert_yaxis()\nax.xaxis.tick_top()\n\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### R Visualizations"],"metadata":{}},{"cell_type":"code","source":["%r\n# Install necessary packages to use ggplot2 \ninstall.packages(\"ggplot2\")\ninstall.packages(\"reshape\")\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(ggplot2)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%r\nscores <- sql(\"FROM popular_posts_2014 SELECT *\")\nlocal_df <- collect(scores)\n\n# We can pivot the data in 2 ways, option 1 commented out\n# local_df$day <- factor(local_df$day)\n# xtabs(score_gt_2k ~ hour+day, local_df)\n\nheat_val <- with(local_df, tapply(score_gt_1k, list(hour, day) , I)  )\n# Define logical times\ntimes <- c(\"12 AM\", \"1 AM\", \"2 AM\", \"3 AM\", \"4 AM\", \"5 AM\", \"6 AM\", \"7 AM\", \"8 AM\", \"9 AM\", \"10 AM\", \"11 AM\", \"12 PM\", \"1 PM\", \"2 PM\", \"3 PM\", \"4 PM\", \"5 PM\", \"6 PM\", \"7 PM\", \"8 PM\", \"9 PM\", \"10 PM\", \"11 PM\")\nheat_val[times, ]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%r\n# Testing out the factor api, which doesn't do much until you use ggplot \nlocal_df.m <- melt(local_df)\nlocal_df.m$hour <- factor(local_df.m$hour, levels=times)\nlocal_df.m"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%r\nlibrary(scales)\n# Melt flattens the R.DataFrame into a friendly format for ggplot\nlocal_df.m <- melt(local_df)\n# factor() allows you to specify the exact ordering for the values within a column! This is extremely important since these values have no machine readable sort order. \nlocal_df.m$hour <- factor(local_df.m$hour, levels=rev(times))\nlocal_df.m$day <- factor(local_df.m$day, levels=c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\n# This provides the heatmap of the comment posts\np <- ggplot(local_df.m, aes(day, hour)) + geom_tile(aes(fill = value), colour = \"white\") + scale_fill_gradient(low = \"white\", high = \"steelblue\")\np"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### Look at the Average Length of Comments vs Comment Score"],"metadata":{}},{"cell_type":"code","source":["df = table(\"reddit_2010\").unionAll(table(\"reddit_2011\")).unionAll(table(\"reddit_2012\"))\ndf.registerTempTable(\"reddit_201x\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["dfc = sqlContext.sql(\"\"\"SELECT\n  score,\n  AVG(LENGTH(body)) as avg_comment_length,\n  STDDEV(LENGTH(body))/SQRT(COUNT(score)) as se_comment_length,\n  COUNT(score) as num_comments\n FROM reddit_201x\n GROUP BY score\n ORDER BY score\"\"\") \n\ndf = dfc.filter(\"score >= -200 and score <=2000\").select(\"score\", \"avg_comment_length\", \"se_comment_length\").toPandas()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from ggplot import *\n\np = ggplot(df, aes(x='score', y='avg_comment_length')) + \\\n    geom_line(size=0.25, color='red') + \\\n    ylim(0, 1100) + \\\n    xlim(-200, 2000) + \\\n    xlab(\"(# Upvotes - # Downvotes)\") + \\\n    ylab(\"Avg. Length of Comment For Each Score\") + \\\n    ggtitle(\"Relationship between Reddit Comment Score and Comment Length for Comments\")\n    \ndisplay(p)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Find the Most Active Community in SubReddits"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT subreddit, num_comments \n  FROM (\n    SELECT count(*) as num_comments, \n          subreddit \n    FROM reddit_2014 \n    GROUP BY subreddit\n    ORDER BY num_comments DESC\n    LIMIT 20\n  ) t1 "],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"Reddit SQL Analysis","notebookId":244782},"nbformat":4,"nbformat_minor":0}